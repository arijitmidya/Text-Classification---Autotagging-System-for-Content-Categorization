{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project: Auto-Tagging System for Content Categorization\n",
    "\n",
    "In this project, you’ll use Python modules and spaCy—a cutting-edge NLP library—to develop an automated system for tagging textual content efficiently."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1: Import Libraries and Modules\n",
    "\n",
    "In this project, you’ll use the following Python libraries. Start the project by importing the necessary libraries and modules into the Jupyter Notebook.\n",
    "\n",
    "1. spacy: It’s a powerful Python library for advanced NLP tasks.\n",
    "\n",
    "2. re: It helps define regular expression patterns for text.\n",
    "\n",
    "3. pandas: It’s useful for dealing with DataFrames.\n",
    "\n",
    "4. nltk: It helps access some helper functions for text preprocessing.\n",
    "\n",
    "5. Matcher: It’s a module in spacy that helps define rule-based entities or any technical terms.\n",
    "\n",
    "6. contractions: It helps handle contractions in text processing steps.\n",
    "\n",
    "7. random: It helps get a random element from several elements.\n",
    "\n",
    "8. Example: It’s a class that’s part of the spacy.training module and helps in fine-tuning the spacy model.\n",
    "\n",
    "9. Counter: It’s a module in the collections class that helps count hashable objects.\n",
    "\n",
    "10. ast: It’s a tool for understanding and changing Python code.\n",
    "\n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import re\n",
    "import pandas as pd\n",
    "import contractions\n",
    "import nltk\n",
    "from spacy.matcher import Matcher\n",
    "from spacy.training import Example\n",
    "import random\n",
    "from collections import Counter\n",
    "import ast"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2: Load and Explore the Dataset\n",
    "\n",
    "The data for this project is from the Newsgroups dataset. This dataset is a popular choice for experiments in text applications of machine learning, \n",
    "specifically for tasks like content categorization and classification. Each document in the dataset represents a newsgroup post, making it a rich \n",
    "resource for understanding and working with real-world text data in NLP. The dataset has only a single News Data named column.\n",
    "\n",
    "To complete this task, perform the following steps:\n",
    "\n",
    "1. Read the data from the news_data.csv named CSV file, which is located in the /usercode directory.\n",
    "\n",
    "2. Load the dataset and print the head of the dataset.\n",
    "\n",
    "3. Print the first news data row of the dataset.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                           News Data\n",
      "0  From: dlphknob@camelot.bradley.edu (Jemaleddin...\n",
      "1  From: svoboda@rtsg.mot.com (David Svoboda)\\nSu...\n",
      "2  From: ld231782@longs.lance.colostate.edu (L. D...\n",
      "3  From: ipser@solomon.technet.sg (Ed Ipser)\\nSub...\n",
      "4  From: cme@ellisun.sw.stratus.com (Carl Ellison...\n",
      "From: dlphknob@camelot.bradley.edu (Jemaleddin Cole)\n",
      "Subject: Re: Catholic Lit-Crit of a.s.s.\n",
      "Nntp-Posting-Host: camelot.bradley.edu\n",
      "Organization: The Society for the Preservation of Cruelty to Homophobes.\n",
      "Lines: 37\n",
      "\n",
      "In <1993Apr14.101241.476@mtechca.maintech.com> foster@mtechca.maintech.com writes:\n",
      "\n",
      ">I am surprised and saddened. I would expect this kind of behavior\n",
      ">from the Evangelical Born-Again Gospel-Thumping In-Your-Face We're-\n",
      ">The-Only-True-Christian Protestants, but I have always thought \n",
      ">that Catholics behaved better than this.\n",
      ">                                   Please do not stoop to the\n",
      ">level of the E B-A G-T I-Y-F W-T-O-T-C Protestants, who think\n",
      ">that the best way to witness is to be strident, intrusive, loud,\n",
      ">insulting and overbearingly self-righteous.\n",
      "\n",
      "(Pleading mode on)\n",
      "\n",
      "Please!  I'm begging you!  Quit confusing religious groups, and stop\n",
      "making generalizations!  I'm a Protestant!  I'm an evangelical!  I don't\n",
      "believe that my way is the only way!  I'm not a \"creation scientist\"!  I\n",
      "don't think that homosexuals should be hung by their toenails!  \n",
      "\n",
      "If you want to discuss bible thumpers, you would be better off singling\n",
      "out (and making obtuse generalizations about) Fundamentalists.  If you\n",
      "compared the actions of Presbyterians or Methodists with those of Southern \n",
      "Baptists, you would think that they were different religions!\n",
      "\n",
      "Please, prejudice is about thinking that all people of a group are the\n",
      "same, so please don't write off all Protestants or all evangelicals!\n",
      "\n",
      "(Pleading mode off.)\n",
      "\n",
      "God.......I wish I could get ahold of all the Thomas Stories......\n",
      "--\n",
      "\t\"Fbzr enval jvagre Fhaqnlf jura gurer'f n yvggyr oberqbz, lbh fubhyq\n",
      "nyjnlf pneel n tha.  Abg gb fubbg lbhefrys, ohg gb xabj rknpgyl gung lbh'er \n",
      "nyjnlf znxvat n pubvpr.\"\n",
      "\t\t\t--Yvan Jregzhyyre\n",
      "=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "        Jemaleddin Sasha David Cole IV - Chief of Knobbery Research\n",
      "                        dlphknob@camelot.bradley.edu\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# loading news data\n",
    "df = pd.read_csv('/usercode/news_data.csv')\n",
    "\n",
    "print(df.head())\n",
    "print(df[\"News Data\"].iloc[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3: Handle Text Case, Contractions, and URLs\n",
    "\n",
    "In the previous task, you observed how messy the data is for further analysis. Now, you’ll explore a set of data processing and cleaning approaches\n",
    "that will clean the data.\n",
    "\n",
    "To complete this task, perform the following steps:\n",
    "\n",
    "1. Extract random text data from the news_data dataset.\n",
    "\n",
    "2. Write a convert_to_lowercase() function that converts this text into lowercase and prints the updated text.\n",
    "\n",
    "3. Write another expand_contractions() function that handles any contractions available in the text and prints the updated text.\n",
    "\n",
    "4. Write another remove_urls() function that detects and removes the URLs in the text and prints the updated text.\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = df[\"News Data\"].iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "from: dlphknob@camelot.bradley.edu (jemaleddin cole)\n",
      "subject: re: catholic lit-crit of a.s.s.\n",
      "nntp-posting-host: camelot.bradley.edu\n",
      "organization: the society for the preservation of cruelty to homophobes.\n",
      "lines: 37\n",
      "\n",
      "in <1993apr14.101241.476@mtechca.maintech.com> foster@mtechca.maintech.com writes:\n",
      "\n",
      ">i am surprised and saddened. i would expect this kind of behavior\n",
      ">from the evangelical born-again gospel-thumping in-your-face we're-\n",
      ">the-only-true-christian protestants, but i have always thought \n",
      ">that catholics behaved better than this.\n",
      ">                                   please do not stoop to the\n",
      ">level of the e b-a g-t i-y-f w-t-o-t-c protestants, who think\n",
      ">that the best way to witness is to be strident, intrusive, loud,\n",
      ">insulting and overbearingly self-righteous.\n",
      "\n",
      "(pleading mode on)\n",
      "\n",
      "please!  i'm begging you!  quit confusing religious groups, and stop\n",
      "making generalizations!  i'm a protestant!  i'm an evangelical!  i don't\n",
      "believe that my way is the only way!  i'm not a \"creation scientist\"!  i\n",
      "don't think that homosexuals should be hung by their toenails!  \n",
      "\n",
      "if you want to discuss bible thumpers, you would be better off singling\n",
      "out (and making obtuse generalizations about) fundamentalists.  if you\n",
      "compared the actions of presbyterians or methodists with those of southern \n",
      "baptists, you would think that they were different religions!\n",
      "\n",
      "please, prejudice is about thinking that all people of a group are the\n",
      "same, so please don't write off all protestants or all evangelicals!\n",
      "\n",
      "(pleading mode off.)\n",
      "\n",
      "god.......i wish i could get ahold of all the thomas stories......\n",
      "--\n",
      "\t\"fbzr enval jvagre fhaqnlf jura gurer'f n yvggyr oberqbz, lbh fubhyq\n",
      "nyjnlf pneel n tha.  abg gb fubbg lbhefrys, ohg gb xabj rknpgyl gung lbh'er \n",
      "nyjnlf znxvat n pubvpr.\"\n",
      "\t\t\t--yvan jregzhyyre\n",
      "=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "        jemaleddin sasha david cole iv - chief of knobbery research\n",
      "                        dlphknob@camelot.bradley.edu\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def convert_to_lowercase(test_data): \n",
    "    return test_data.lower()\n",
    "\n",
    "test_data = convert_to_lowercase(test_data)\n",
    "\n",
    "print(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "from: dlphknob@camelot.bradley.edu (jemaleddin cole)\n",
      "subject: re: catholic lit-crit of a.s.s.\n",
      "nntp-posting-host: camelot.bradley.edu\n",
      "organization: the society for the preservation of cruelty to homophobes.\n",
      "lines: 37\n",
      "\n",
      "in <1993apr14.101241.476@mtechca.maintech.com> foster@mtechca.maintech.com writes:\n",
      "\n",
      ">i am surprised and saddened. i would expect this kind of behavior\n",
      ">from the evangelical born-again gospel-thumping in-your-face we are-\n",
      ">the-only-true-christian protestants, but i have always thought \n",
      ">that catholics behaved better than this.\n",
      ">                                   please do not stoop to the\n",
      ">level of the e b-a g-t i-y-f w-t-o-t-c protestants, who think\n",
      ">that the best way to witness is to be strident, intrusive, loud,\n",
      ">insulting and overbearingly self-righteous.\n",
      "\n",
      "(pleading mode on)\n",
      "\n",
      "please!  i am begging you!  quit confusing religious groups, and stop\n",
      "making generalizations!  i am a protestant!  i am an evangelical!  i do not\n",
      "believe that my way is the only way!  i am not a \"creation scientist\"!  i\n",
      "do not think that homosexuals should be hung by their toenails!  \n",
      "\n",
      "if you want to discuss bible thumpers, you would be better off singling\n",
      "out (and making obtuse generalizations about) fundamentalists.  if you\n",
      "compared the actions of presbyterians or methodists with those of southern \n",
      "baptists, you would think that they were different religions!\n",
      "\n",
      "please, prejudice is about thinking that all people of a group are the\n",
      "same, so please do not write off all protestants or all evangelicals!\n",
      "\n",
      "(pleading mode off.)\n",
      "\n",
      "god.......i wish i could get ahold of all the thomas stories......\n",
      "--\n",
      "\t\"fbzr enval jvagre fhaqnlf jura gurer'f n yvggyr oberqbz, lbh fubhyq\n",
      "nyjnlf pneel n tha.  abg gb fubbg lbhefrys, ohg gb xabj rknpgyl gung lbh'er \n",
      "nyjnlf znxvat n pubvpr.\"\n",
      "\t\t\t--yvan jregzhyyre\n",
      "=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "        jemaleddin sasha david cole iv - chief of knobbery research\n",
      "                        dlphknob@camelot.bradley.edu\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def expand_contractions(test_data):\n",
    "    return contractions.fix(test_data)\n",
    "\n",
    "test_data = expand_contractions(test_data)\n",
    "\n",
    "print(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "from: dlphknob@camelot.bradley.edu (jemaleddin cole)\n",
      "subject: re: catholic lit-crit of a.s.s.\n",
      "nntp-posting-host: camelot.bradley.edu\n",
      "organization: the society for the preservation of cruelty to homophobes.\n",
      "lines: 37\n",
      "\n",
      "in <1993apr14.101241.476@mtechca.maintech.com> foster@mtechca.maintech.com writes:\n",
      "\n",
      ">i am surprised and saddened. i would expect this kind of behavior\n",
      ">from the evangelical born-again gospel-thumping in-your-face we are-\n",
      ">the-only-true-christian protestants, but i have always thought \n",
      ">that catholics behaved better than this.\n",
      ">                                   please do not stoop to the\n",
      ">level of the e b-a g-t i-y-f w-t-o-t-c protestants, who think\n",
      ">that the best way to witness is to be strident, intrusive, loud,\n",
      ">insulting and overbearingly self-righteous.\n",
      "\n",
      "(pleading mode on)\n",
      "\n",
      "please!  i am begging you!  quit confusing religious groups, and stop\n",
      "making generalizations!  i am a protestant!  i am an evangelical!  i do not\n",
      "believe that my way is the only way!  i am not a \"creation scientist\"!  i\n",
      "do not think that homosexuals should be hung by their toenails!  \n",
      "\n",
      "if you want to discuss bible thumpers, you would be better off singling\n",
      "out (and making obtuse generalizations about) fundamentalists.  if you\n",
      "compared the actions of presbyterians or methodists with those of southern \n",
      "baptists, you would think that they were different religions!\n",
      "\n",
      "please, prejudice is about thinking that all people of a group are the\n",
      "same, so please do not write off all protestants or all evangelicals!\n",
      "\n",
      "(pleading mode off.)\n",
      "\n",
      "god.......i wish i could get ahold of all the thomas stories......\n",
      "--\n",
      "\t\"fbzr enval jvagre fhaqnlf jura gurer'f n yvggyr oberqbz, lbh fubhyq\n",
      "nyjnlf pneel n tha.  abg gb fubbg lbhefrys, ohg gb xabj rknpgyl gung lbh'er \n",
      "nyjnlf znxvat n pubvpr.\"\n",
      "\t\t\t--yvan jregzhyyre\n",
      "=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "        jemaleddin sasha david cole iv - chief of knobbery research\n",
      "                        dlphknob@camelot.bradley.edu\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def remove_urls(test_data): \n",
    "    url_pattern = r'https?://\\S+|www\\.\\S+'\n",
    "    return re.sub(url_pattern, ' ', test_data)\n",
    "\n",
    "test_data = remove_urls(test_data)\n",
    "\n",
    "print(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 4: Handle Emails and Datetime\n",
    "\n",
    "In the previous task, you handled the text casing and URLs in the text data. Now, you’ll handle emails and date time elements present in the data. \n",
    "This is an incremental approach to text processing, which you must follow in order.\n",
    "\n",
    "To complete this task, perform the following steps:\n",
    "\n",
    "1. Create a remove_email_addresses() function to detect and remove email addresses from the text and print the updated text. This text should be free \n",
    "from any email addresses.\n",
    "\n",
    "2. Create a remove_dates_times() function to detect and remove date time elements from the text and print the updated text. This text should be free\n",
    "from any date time elements.\n",
    "\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_email_addresses(test_data):\n",
    "    email_pattern = r'\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,}\\b'\n",
    "    return re.sub(email_pattern, ' ', test_data)\n",
    "\n",
    "test_data = remove_email_addresses(test_data)\n",
    "test_data = re.sub(r'\\s{2,}',' ', test_data).strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "from: (jemaleddin cole)\n",
      "subject: re: catholic lit-crit of a.s.s.\n",
      "nntp-posting-host: camelot.bradley.edu\n",
      "organization: the society for the preservation of cruelty to homophobes.\n",
      "lines: 37 in < > writes: >i am surprised and saddened. i would expect this kind of behavior\n",
      ">from the evangelical born-again gospel-thumping in-your-face we are-\n",
      ">the-only-true-christian protestants, but i have always thought >that catholics behaved better than this.\n",
      "> please do not stoop to the\n",
      ">level of the e b-a g-t i-y-f w-t-o-t-c protestants, who think\n",
      ">that the best way to witness is to be strident, intrusive, loud,\n",
      ">insulting and overbearingly self-righteous. (pleading mode on) please! i am begging you! quit confusing religious groups, and stop\n",
      "making generalizations! i am a protestant! i am an evangelical! i do not\n",
      "believe that my way is the only way! i am not a \"creation scientist\"! i\n",
      "do not think that homosexuals should be hung by their toenails! if you want to discuss bible thumpers, you would be better off singling\n",
      "out (and making obtuse generalizations about) fundamentalists. if you\n",
      "compared the actions of presbyterians or methodists with those of southern baptists, you would think that they were different religions! please, prejudice is about thinking that all people of a group are the\n",
      "same, so please do not write off all protestants or all evangelicals! (pleading mode off.) god.......i wish i could get ahold of all the thomas stories......\n",
      "-- \"fbzr enval jvagre fhaqnlf jura gurer'f n yvggyr oberqbz, lbh fubhyq\n",
      "nyjnlf pneel n tha. abg gb fubbg lbhefrys, ohg gb xabj rknpgyl gung lbh'er nyjnlf znxvat n pubvpr.\" --yvan jregzhyyre\n",
      "=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-= jemaleddin sasha david cole iv - chief of knobbery research\n"
     ]
    }
   ],
   "source": [
    "def remove_dates_times(test_data): \n",
    "    date_pattern = r'\\b\\d{1,2}[/-]\\d{1,2}[/-]\\d{2,4}\\b'\n",
    "    time_pattern = r'\\b\\d{1,2}:\\d{2}(?: \\s?[AP]M)?\\b' \n",
    "    test_data = re.sub(date_pattern, ' ', test_data) \n",
    "    test_data = re.sub(time_pattern, ' ', test_data) \n",
    "    return test_data\n",
    "\n",
    "test_data = remove_dates_times(test_data)\n",
    "test_data = re.sub(r'\\s{2,}',' ', test_data).strip()\n",
    "\n",
    "print(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 5: Remove Numbers and Special Characters\n",
    "\n",
    "In the previous task, you removed emails and date time elements present in the text data. Now, you’ll handle special characters and numbers.\n",
    "\n",
    "To complete this task, perform the following steps:\n",
    "\n",
    "1. Create a remove_numbers_and_special_characters() named function to detect and remove both numbers and special characters present in the data.\n",
    "\n",
    "2. Apply the function to your sample text and print the updated text.\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "from jemaleddin cole subject re catholic lit crit of a.s.s. nntp posting host camelot.bradley.edu organization the society for the preservation of cruelty to homophobes. lines in writes i am surprised and saddened. i would expect this kind of behavior from the evangelical born again gospel thumping in your face we are the only true christian protestants but i have always thought that catholics behaved better than this. please do not stoop to the level of the e b a g t i y f w t o t c protestants who think that the best way to witness is to be strident intrusive loud insulting and overbearingly self righteous. pleading mode on please i am begging you quit confusing religious groups and stop making generalizations i am a protestant i am an evangelical i do not believe that my way is the only way i am not a creation scientist i do not think that homosexuals should be hung by their toenails if you want to discuss bible thumpers you would be better off singling out and making obtuse generalizations about fundamentalists. if you compared the actions of presbyterians or methodists with those of southern baptists you would think that they were different religions please prejudice is about thinking that all people of a group are the same so please do not write off all protestants or all evangelicals pleading mode off. god.......i wish i could get ahold of all the thomas stories...... fbzr enval jvagre fhaqnlf jura gurer f n yvggyr oberqbz lbh fubhyq nyjnlf pneel n tha. abg gb fubbg lbhefrys ohg gb xabj rknpgyl gung lbh er nyjnlf znxvat n pubvpr. yvan jregzhyyre jemaleddin sasha david cole iv chief of knobbery research\n"
     ]
    }
   ],
   "source": [
    "def remove_numbers_and_special_characters(test_data):\n",
    "    number_pattern = r'\\b\\d+\\b'\n",
    "    special_char_pattern = r'[^\\w\\s\\.]|_|\\s+'\n",
    "    test_data =  re.sub(number_pattern, ' ', test_data)\n",
    "    test_data = re.sub(r'\\s{2,}',' ', test_data).strip()\n",
    "    return re.sub(special_char_pattern, ' ', test_data)\n",
    "\n",
    "test_data = remove_numbers_and_special_characters(test_data)\n",
    "test_data = re.sub(r'\\s{2,}',' ', test_data).strip()\n",
    "\n",
    "print(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 6: Handle Stopwords and Extra Spaces\n",
    "\n",
    "In this task, you’ll remove stop words and extra spaces from the text.\n",
    "\n",
    "To complete this task, perform the following steps:\n",
    "\n",
    "1. Create a remove_stop_words_and_spaces() named function that uses the list of stopwords to check if any of these stopwords are present in the text \n",
    "and remove them if present.\n",
    "\n",
    "2. Remove any extra spaces present in the text.\n",
    "\n",
    "3. Print the final cleaned text.\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jemaleddin cole subject catholic lit crit a.s.s . nntp posting host camelot.bradley.edu organization society preservation cruelty homophobes . lines writes surprised saddened . expect kind behavior evangelical born gospel thumping face true christian protestants thought catholics behaved better . stoop level e b g t y f w t o t c protestants think best way witness strident intrusive loud insulting overbearingly self righteous . pleading mode begging quit confusing religious groups stop making generalizations protestant evangelical believe way way creation scientist think homosexuals hung toenails want discuss bible thumpers better singling making obtuse generalizations fundamentalists . compared actions presbyterians methodists southern baptists think different religions prejudice thinking people group write protestants evangelicals pleading mode . god ....... wish ahold thomas stories ...... fbzr enval jvagre fhaqnlf jura gurer f n yvggyr oberqbz lbh fubhyq nyjnlf pneel n tha . abg gb fubbg lbhefrys ohg gb xabj rknpgyl gung lbh er nyjnlf znxvat n pubvpr . yvan jregzhyyre jemaleddin sasha david cole iv chief knobbery research\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.load('en_core_web_md', disable=[ 'parser', 'lemmatizer', 'attribute_ruler'])\n",
    "\n",
    "def remove_stop_words_and_spaces(test_data, nlp_model):\n",
    "    doc = nlp_model(test_data)\n",
    "    filtered_text = [token.text for token in doc if not token.is_stop]\n",
    "    return(' '.join(filtered_text))\n",
    "\n",
    "# Assuming nlp is defined elsewhere and passed appropriately\n",
    "test_data = remove_stop_words_and_spaces(test_data, nlp)\n",
    "\n",
    "print(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 7: Tokenize Cleaned Text\n",
    "\n",
    "Tokenization is a crucial step in NLP, where text is split into words or phrases called tokens. This process is essential for understanding the \n",
    "structure and meaning of text. Tokenization allows algorithms to process and analyze text at a granular level.\n",
    "\n",
    "Tokenization algorithms built into spaCy perform tokenization tasks well. Its models are optimized for speed and accuracy, which makes spaCy ideal \n",
    "for processing large volumes of text efficiently.\n",
    "\n",
    "To complete this task, perform the following steps:\n",
    "\n",
    "1. Create a tokenize_text() named function that takes the preprocessed version of the sample text and returns the list of tokens as an output.\n",
    "\n",
    "2. Print the list of tokens.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['jemaleddin', 'cole', 'subject', 'catholic', 'lit', 'crit', 'a.s.s', '.', 'nntp', 'posting', 'host', 'camelot.bradley.edu', 'organization', 'society', 'preservation', 'cruelty', 'homophobes', '.', 'lines', 'writes', 'surprised', 'saddened', '.', 'expect', 'kind', 'behavior', 'evangelical', 'born', 'gospel', 'thumping', 'face', 'true', 'christian', 'protestants', 'thought', 'catholics', 'behaved', 'better', '.', 'stoop', 'level', 'e', 'b', 'g', 't', 'y', 'f', 'w', 't', 'o', 't', 'c', 'protestants', 'think', 'best', 'way', 'witness', 'strident', 'intrusive', 'loud', 'insulting', 'overbearingly', 'self', 'righteous', '.', 'pleading', 'mode', 'begging', 'quit', 'confusing', 'religious', 'groups', 'stop', 'making', 'generalizations', 'protestant', 'evangelical', 'believe', 'way', 'way', 'creation', 'scientist', 'think', 'homosexuals', 'hung', 'toenails', 'want', 'discuss', 'bible', 'thumpers', 'better', 'singling', 'making', 'obtuse', 'generalizations', 'fundamentalists', '.', 'compared', 'actions', 'presbyterians', 'methodists', 'southern', 'baptists', 'think', 'different', 'religions', 'prejudice', 'thinking', 'people', 'group', 'write', 'protestants', 'evangelicals', 'pleading', 'mode', '.', 'god', '.......', 'wish', 'ahold', 'thomas', 'stories', '......', 'fbzr', 'enval', 'jvagre', 'fhaqnlf', 'jura', 'gurer', 'f', 'n', 'yvggyr', 'oberqbz', 'lbh', 'fubhyq', 'nyjnlf', 'pneel', 'n', 'tha', '.', 'abg', 'gb', 'fubbg', 'lbhefrys', 'ohg', 'gb', 'xabj', 'rknpgyl', 'gung', 'lbh', 'er', 'nyjnlf', 'znxvat', 'n', 'pubvpr', '.', 'yvan', 'jregzhyyre', 'jemaleddin', 'sasha', 'david', 'cole', 'iv', 'chief', 'knobbery', 'research']\n"
     ]
    }
   ],
   "source": [
    "def tokenize_text(text):\n",
    "    tokens = [token.text for token in nlp(text)]\n",
    "    return tokens\n",
    "\n",
    "tokens = tokenize_text(test_data)\n",
    "\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 8: Build Data Prep Pipeline\n",
    "\n",
    "In this task, you’ll create a data preparation pipeline that takes raw text data (e.g., the complete News Data column) as input and returns the\n",
    "preprocessed and tokenized data. For this, you’ll utilize all the preprocessing steps that you’ve done so far.\n",
    "\n",
    "To complete this task, perform the following steps:\n",
    "\n",
    "1. Create a preprocess_data() named function that should utilize all the data preprocessing functions to preprocess the dataset and return the processed\n",
    "data.\n",
    "\n",
    "2. Create a preprocess_and_tokenize() named function that should tokenize the processed data.\n",
    "\n",
    "3. Apply the preprocess_data() and preprocess_and_tokenize() functions to the News Data column in the DataFrame such that for each data item, you should \n",
    "have both processed data and the processed tokenized data version.\n",
    "\n",
    "4. Print the head of the new DataFrame.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(text):\n",
    "    # Apply all preprocessing functions\n",
    "    text = convert_to_lowercase(text)\n",
    "    text = expand_contractions(text)\n",
    "    text = remove_urls(text)\n",
    "    text = remove_email_addresses(text)\n",
    "    text = remove_dates_times(text)\n",
    "    text = remove_numbers_and_special_characters(text)\n",
    "    text = remove_stop_words_and_spaces(text, nlp)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_and_tokenize(text):\n",
    "    tokens = tokenize_text(text)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                           News Data  \\\n",
      "0  From: dlphknob@camelot.bradley.edu (Jemaleddin...   \n",
      "1  From: svoboda@rtsg.mot.com (David Svoboda)\\nSu...   \n",
      "2  From: ld231782@longs.lance.colostate.edu (L. D...   \n",
      "3  From: ipser@solomon.technet.sg (Ed Ipser)\\nSub...   \n",
      "4  From: cme@ellisun.sw.stratus.com (Carl Ellison...   \n",
      "\n",
      "                                      Processed_Data  \\\n",
      "0     jemaleddin cole   subject     catholic lit ...   \n",
      "1     david svoboda   subject     opinion means ....   \n",
      "2     l. detweiler   subject   privacy    anonymi...   \n",
      "3     ed ipser   subject   ways slick willie impr...   \n",
      "4     carl ellison   subject     clipper crypto o...   \n",
      "\n",
      "                            Processed_Tokenized_Data  \n",
      "0  [   , jemaleddin, cole,   , subject,     , cat...  \n",
      "1  [   , david, svoboda,   , subject,     , opini...  \n",
      "2  [   , l., detweiler,   , subject,   , privacy,...  \n",
      "3  [   , ed, ipser,   , subject,   , ways, slick,...  \n",
      "4  [   , carl, ellison,   , subject,     , clippe...  \n"
     ]
    }
   ],
   "source": [
    "df['Processed_Data'] = df['News Data'].apply(preprocess_data)\n",
    "\n",
    "df['Processed_Tokenized_Data'] = df['Processed_Data'].apply(preprocess_and_tokenize)\n",
    "\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 9: Create Pattern Matching Flow\n",
    "\n",
    "The Matcher class in spaCy allows you to define patterns based on token attributes like text, part-of-speech tags, and syntactic dependencies. This\n",
    "class uses a dictionary of patterns. In the dictionary, the keys represent entities or technical term names, and the values are the pattern \n",
    "definitions you want to identify.\n",
    "\n",
    "For instance, if you have a {\"GPE\": [{\"LOWER\": \"India\"}]} like pattern, it means you’re looking for occurrences of the word India in lowercase, \n",
    "and when found, it’s labeled as a GPE (geopolitical entity).\n",
    "\n",
    "To complete this task, perform the following steps:\n",
    "\n",
    "1. Define a sample_text variable with the following value:\n",
    "I invested $1000 on 11 Jan 2021, and it grew to $5 million by Dec 2022.\n",
    "\n",
    "2. Define a patterns dictionary with patterns so that you can identify the date and currency elements present in the sample_text string.\n",
    "\n",
    "3. Create a find_matches() named function which takes the sample_text string and the patterns dictionary as input and returns the identified \n",
    "terms in the data.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "$1000 - MONEY_PATTERN\n",
      "11 Jan - DATE_PATTERN\n",
      "11 Jan 2021 - DATE_PATTERN\n",
      "Jan - DATE_PATTERN\n",
      "Jan 2021 - DATE_PATTERN\n",
      "$5 - MONEY_PATTERN\n",
      "$5 million - MONEY_PATTERN\n",
      "Dec - DATE_PATTERN\n",
      "Dec 2022 - DATE_PATTERN\n"
     ]
    }
   ],
   "source": [
    "# Define a function to find matches in a text\n",
    "def find_matches(text, patterns):\n",
    "    # Create a Matcher object\n",
    "    matcher = Matcher(nlp.vocab)\n",
    "\n",
    "    # Add patterns to the matcher object\n",
    "    for pattern_name, pattern in patterns.items():\n",
    "        matcher.add(pattern_name, [pattern])\n",
    "\n",
    "    doc = nlp(text)\n",
    "    matches = matcher(doc)\n",
    "    matched_entities = []\n",
    "\n",
    "    for match_id, start, end in matches:\n",
    "        matched_entities.append((doc[start:end].text, nlp.vocab.strings[match_id]))\n",
    "\n",
    "    return matched_entities\n",
    "\n",
    "patterns = {\n",
    "    \"DATE_PATTERN\": [\n",
    "        {\"IS_DIGIT\": True, \"LENGTH\": 2, \"OP\": \"?\"},  # Day as two digits\n",
    "        {\"LOWER\": {\"IN\": [\"jan\", \"feb\", \"mar\", \"apr\", \"may\", \"jun\", \n",
    "                          \"jul\", \"aug\", \"sep\", \"oct\", \"nov\", \"dec\"]}},  # Month abbreviations\n",
    "        {\"IS_DIGIT\": True, \"LENGTH\": 4, \"OP\": \"?\"}  # Four digit year\n",
    "    ],\n",
    "    \"MONEY_PATTERN\": [\n",
    "        {\"ORTH\": \"$\"},  # Optional dollar sign\n",
    "        {\"LIKE_NUM\": True},  # Numeric value\n",
    "        {\"LOWER\": {\"IN\": [\"thousand\", \"million\", \"billion\"]}, \"OP\": \"?\"}  # Large amount modifiers\n",
    "    ]\n",
    "}\n",
    "\n",
    "sample_text = \"I invested $1000 on 11 Jan 2021 and it grew to $5 million by Dec 2022.\"\n",
    "\n",
    "matched_entities = find_matches(sample_text, patterns)\n",
    "\n",
    "for entity, label in matched_entities:\n",
    "    print(f\"{entity} - {label}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 10: Entity Extraction Using spaCy Model\n",
    "\n",
    "In this task, you’ll use spaCy’s model to extract entities from your cleaned text data. You’ll learn to leverage spaCy’s pretrained \n",
    "Named Entity Recognition (NER) model’s capabilities to identify and tag various entities, like names, places, organizations, etc., in your text.\n",
    "\n",
    "spaCy has different types of pretrained models that are of different sizes. You’ll be working with spaCy’s medium model, en_core_web_md.\n",
    "\n",
    "To complete this task, perform the following steps:\n",
    "\n",
    "1. Create a get_entities_medium_spacy() named function, which takes any sample text from your processed data and returns a list of extracted entities.\n",
    "\n",
    "2. Print the extracted entities.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'PERSON': ['tony jones', 'erik asphaug x2773      ', 'tony', 'tony jones'], 'ORG': ['nntp', 'cray research inc   eagan   mn x', 'cmcs codegeneration group', 'cray research inc   '], 'NORP': ['pl6'], 'GPE': ['655f']}\n"
     ]
    }
   ],
   "source": [
    "# Take any sample processed text of your choice\n",
    "sample_text = df[\"Processed_Data\"].iloc[122]\n",
    "\n",
    "def get_entities_medium_spacy(text):\n",
    "    doc = nlp(text)\n",
    "    entities = {}\n",
    "    for ent in doc.ents:\n",
    "        if ent.label_ not in entities:\n",
    "            entities[ent.label_] = []\n",
    "        entities[ent.label_].append(ent.text)\n",
    "    return entities\n",
    "\n",
    "entities = get_entities_medium_spacy(sample_text)\n",
    "\n",
    "print(entities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'   tony jones   subject     insurance discount lines   nntp posting host   palm21 organization   cray research inc   eagan   mn x newsreader   tin   version . pl6   erik asphaug x2773      wrote     ... insurance agent offers multi vehicle discount .    time cars   assuming capable progressive offers multi vehicle discounts . good prices imho . tony     tony jones      .. uunet cray ant   cmcs codegeneration group   software division cray research inc   655f lone oak drive   eagan   mn'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Processed_Data'].iloc[122]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 11: Optimizing spaCy Model\n",
    "\n",
    "In this task, you’ll optimize the existing spaCy medium model, en_core_web_md, to better suit your specific NLP needs. This will involve\n",
    "fine-tuning the model on your dataset, which allows the model to identify and classify entities relevant to your text data more accurately.\n",
    "\n",
    "In the first example, the position of the Noida entity is defined. Here, the start and end+1 indexes are used to define any entity.\n",
    "Similarly, you can add more examples to this list to create custom data.\n",
    "\n",
    "To complete this task, perform the following steps:\n",
    "\n",
    "1. Prepare a set of training data for the spaCy model.\n",
    "\n",
    "2. Use the training data to fine-tune the existing spaCy model.\n",
    "\n",
    "3. Use the fine-tuned model to extract entities.\n",
    "\n",
    "4. Save the optimized model to the /usercode directory with the spacy_optimised name.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Annotated training data\n",
    "\n",
    "train_data = [\n",
    "\n",
    "     (\"Noida is a city in India\", {\"entities\": [(0, 5, \"LOC\")]}),\n",
    "     (\"Google launches a new AI research lab in Zurich\", {\"entities\": [(0, 6, \"ORG\"), (37, 43, \"LOC\")]}),\n",
    "     (\"Amazon acquires Twitch for $970 million in 2014\", {\"entities\": [(0, 6, \"ORG\"), (15, 21, \"ORG\"), (35, 42, \"MONEY\"), (46, 50, \"DATE\")]}),\n",
    "     (\"Elon Musk founded SpaceX to revolutionize space travel\", {\"entities\": [(0, 9, \"PERSON\"), (18, 24, \"ORG\")]}),\n",
    "     (\"The Mona Lisa is on display in the Louvre Museum in Paris\", {\"entities\": [(4, 13, \"WORK_OF_ART\"), (34, 47, \"ORG\"), (51, 56, \"LOC\")]}),\n",
    "     (\"The Great Wall of China stretches over 13,000 miles\", {\"entities\": [(4, 22, \"LOC\"), (42, 51, \"QUANTITY\")]}),\n",
    "     (\"IBM introduces Watson, the AI that beat Jeopardy champions\", {\"entities\": [(0, 3, \"ORG\"), (16, 22, \"PERSON\"), (31, 33, \"ORG\")]}),\n",
    "     (\"The Nile River flows through Egypt\", {\"entities\": [(4, 14, \"LOC\"), (28, 33, \"GPE\")]}),\n",
    "     (\"Harvard University was established in 1636\", {\"entities\": [(0, 17, \"ORG\"), (35, 39, \"DATE\")]}),\n",
    "     (\"Mount Everest is the world's highest mountain\", {\"entities\": [(0, 13, \"LOC\")]}),\n",
    "     (\"Julia Roberts stars in the new Netflix series\", {\"entities\": [(0, 12, \"PERSON\"), (31, 38, \"ORG\")]})\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/spacy/training/iob_utils.py:149: UserWarning: [W030] Some entities could not be aligned in the text \"Amazon acquires Twitch for $970 million in 2014\" with entities \"[(0, 6, 'ORG'), (15, 21, 'ORG'), (35, 42, 'MONEY')...\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/spacy/training/iob_utils.py:149: UserWarning: [W030] Some entities could not be aligned in the text \"The Mona Lisa is on display in the Louvre Museum i...\" with entities \"[(4, 13, 'WORK_OF_ART'), (34, 47, 'ORG'), (51, 56,...\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/spacy/training/iob_utils.py:149: UserWarning: [W030] Some entities could not be aligned in the text \"The Nile River flows through Egypt\" with entities \"[(4, 14, 'LOC'), (28, 33, 'GPE')]\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/spacy/training/iob_utils.py:149: UserWarning: [W030] Some entities could not be aligned in the text \"IBM introduces Watson, the AI that beat Jeopardy c...\" with entities \"[(0, 3, 'ORG'), (16, 22, 'PERSON'), (31, 33, 'ORG'...\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/spacy/training/iob_utils.py:149: UserWarning: [W030] Some entities could not be aligned in the text \"Google launches a new AI research lab in Zurich\" with entities \"[(0, 6, 'ORG'), (37, 43, 'LOC')]\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/spacy/training/iob_utils.py:149: UserWarning: [W030] Some entities could not be aligned in the text \"The Great Wall of China stretches over 13,000 mile...\" with entities \"[(4, 22, 'LOC'), (42, 51, 'QUANTITY')]\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/spacy/training/iob_utils.py:149: UserWarning: [W030] Some entities could not be aligned in the text \"Harvard University was established in 1636\" with entities \"[(0, 17, 'ORG'), (35, 39, 'DATE')]\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/spacy/training/iob_utils.py:149: UserWarning: [W030] Some entities could not be aligned in the text \"Julia Roberts stars in the new Netflix series\" with entities \"[(0, 12, 'PERSON'), (31, 38, 'ORG')]\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Update the NER component with new examples\n",
    "\n",
    "ner = nlp.get_pipe('ner')\n",
    "for _, annotations in train_data:\n",
    "   for ent in annotations.get('entities'):\n",
    "      ner.add_label(ent[2])\n",
    "\n",
    "# Disable other pipeline components for training\n",
    "other_pipes = [pipe for pipe in nlp.pipe_names if pipe != 'ner']\n",
    "with nlp.disable_pipes(*other_pipes):\n",
    "   optimizer = nlp.resume_training()\n",
    "   for iteration in range(30):  # Adjust iterations as needed\n",
    "      random.shuffle(train_data)\n",
    "      for text, annotations in train_data:\n",
    "            doc = nlp.make_doc(text)\n",
    "            example = Example.from_dict(doc, annotations)\n",
    "            nlp.update([example], drop=0.5, sgd=optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'LOC': ['Noida']}\n"
     ]
    }
   ],
   "source": [
    "entities = get_entities_medium_spacy(\"Noida is a city\")\n",
    "print(entities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp.to_disk('/usercode/spacy_optimised')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 12: Optimizing Entity Extraction for Auto-Tagging\n",
    "\n",
    "In this task, you’ll streamline your entity extraction by utilizing spaCy’s pipe() function. This method significantly speeds up the processing \n",
    "of large text datasets by batch processing them, which is essential for handling real-world data volumes efficiently.\n",
    "\n",
    "To complete this task, perform the following steps:\n",
    "\n",
    "1. Create an extract_entities_pipe() named function, which uses the Processed_Data column to extract entities of all texts available in respective \n",
    "rows and store them in an entities named new column.\n",
    "\n",
    "2. Print the head of the DataFrame.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                           News Data  \\\n",
      "0  From: dlphknob@camelot.bradley.edu (Jemaleddin...   \n",
      "1  From: svoboda@rtsg.mot.com (David Svoboda)\\nSu...   \n",
      "2  From: ld231782@longs.lance.colostate.edu (L. D...   \n",
      "3  From: ipser@solomon.technet.sg (Ed Ipser)\\nSub...   \n",
      "4  From: cme@ellisun.sw.stratus.com (Carl Ellison...   \n",
      "\n",
      "                                      Processed_Data  \\\n",
      "0     jemaleddin cole   subject     catholic lit ...   \n",
      "1     david svoboda   subject     opinion means ....   \n",
      "2     l. detweiler   subject   privacy    anonymi...   \n",
      "3     ed ipser   subject   ways slick willie impr...   \n",
      "4     carl ellison   subject     clipper crypto o...   \n",
      "\n",
      "                            Processed_Tokenized_Data  \\\n",
      "0  [   , jemaleddin, cole,   , subject,     , cat...   \n",
      "1  [   , david, svoboda,   , subject,     , opini...   \n",
      "2  [   , l., detweiler,   , subject,   , privacy,...   \n",
      "3  [   , ed, ipser,   , subject,   , ways, slick,...   \n",
      "4  [   , carl, ellison,   , subject,     , clippe...   \n",
      "\n",
      "                                            entities  \n",
      "0  [(nntp, ORG), (thomas stories, PERSON), (jura ...  \n",
      "1  [(david svoboda, PERSON), (nntp, ORG), (coroll...  \n",
      "2  [(part3, ORG), (l. detweiler, PERSON), (j. hel...  \n",
      "3  [(solomon.technet.sg, ORG), (senate, ORG), (en...  \n",
      "4  [(carl ellison, PERSON), (micmail, ORG), (carl...  \n"
     ]
    }
   ],
   "source": [
    "def extract_entities_pipe(texts):\n",
    "    entities = []\n",
    "    for doc in nlp.pipe(texts):\n",
    "        entities.append([(ent.text, ent.label_) for ent in doc.ents])\n",
    "    return entities\n",
    "\n",
    "if 'Processed_Data' in df.columns:\n",
    "    df['entities'] = extract_entities_pipe(df['Processed_Data'])\n",
    "    print(df.head())\n",
    "else:\n",
    "    print(\"The 'Processed_Data' column does not exist in the DataFrame.\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 13: Enhanching Entity Aggregation for Workflow Optimization\n",
    "\n",
    "In this task, you’ll enhance your auto-tagging system by aggregating similar entities, reducing redundancy, and ensuring consistency in your tags\n",
    "\n",
    "To complete this task, perform the following steps:\n",
    "\n",
    "1. Create an aggregate_entities() named function that uses the entities column to aggregate similar entities and save the result in a\n",
    "new Refined_Entities named column.\n",
    "\n",
    "2. Print the head of the Refined_Entities column.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    {'ORG': ['nntp'], 'PERSON': ['david cole', 'gb...\n",
      "1    {'PERSON': ['david svoboda', 'dave svoboda', '...\n",
      "2    {'ORG': ['part3', 'n7kbt.rain.com', 'nntp', 'g...\n",
      "3    {'ORG': ['solomon.technet.sg', 'senate'], 'LOC...\n",
      "4     {'PERSON': ['carl ellison'], 'ORG': ['micmail']}\n",
      "Name: Refined_Entities, dtype: object\n"
     ]
    }
   ],
   "source": [
    "def aggregate_entities(entities_list):\n",
    "    aggregated_entities = {}\n",
    "    for ent_text, ent_label in entities_list:\n",
    "        if ent_label in aggregated_entities:\n",
    "            aggregated_entities[ent_label].add(ent_text)\n",
    "        else:\n",
    "            aggregated_entities[ent_label] = {ent_text}\n",
    "    return {label: list(texts) for label, texts in aggregated_entities.items()}\n",
    "\n",
    "df['Refined_Entities'] = df['entities'].apply(aggregate_entities)\n",
    "\n",
    "print(df[\"Refined_Entities\"].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 14: Preparing and Refining Test Data for Entity Analysis\n",
    "\n",
    "In this task, you’ll take a crucial step toward enhancing your entity analysis by preparing the test data. This preparation is important for \n",
    "ensuring that your entity analysis is performed on clean, relevant data, which is fundamental for the accurate tagging process.\n",
    "\n",
    "To complete this task, perform the following steps:\n",
    "\n",
    "1. Utilize the test data (test_data.csv) available in the /usercode directory to test the model. Test data has an Actual_Entities named column, which \n",
    "is a string-formatted dictionary converted to dictionary format.\n",
    "\n",
    "2. Add a new Processed_Data named column in the test_data using the data preprocessing pipeline created earlier.\n",
    "\n",
    "3. Add a new entities named column in test_data with the help of the Processed_Data column, using the extract_entities_pipe() function you created earlier.\n",
    "\n",
    "4. Add a new Refined_Entities named column in test_data using the aggregate_entities() function created earlier.\n",
    "\n",
    "5. Print the head of test_data.\n",
    "\n",
    "                                                                       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>News Data</th>\n",
       "      <th>Actual_Entities</th>\n",
       "      <th>Refined_Entities</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>From: rcollins@ns.encore.com (Roger Collins)\\n...</td>\n",
       "      <td>{'PERSON': ['roger collins', 'clinton', 'steve...</td>\n",
       "      <td>{'PERSON': ['steve hendricks', 'roger collins'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>From: baalke@kelvin.jpl.nasa.gov (Ron Baalke)\\...</td>\n",
       "      <td>{'PERSON': ['daniel burstein', 'ron baalke', '...</td>\n",
       "      <td>{'PERSON': ['ron baalke', 'daniel burstein'], ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Distribution: world\\nFrom: David_A._Schnider@b...</td>\n",
       "      <td>{'ORG': ['mac vga', 'sony cpd', 'cpd', 'trinit...</td>\n",
       "      <td>{'ORG': ['sony'], 'LOC': ['macs'], 'PERSON': [...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>From: brian@nostromo.NoSubdomain.NoDomain (Bri...</td>\n",
       "      <td>{'PERSON': ['brian colaric sun', 'brian colari...</td>\n",
       "      <td>{'ORG': ['os2']}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>From: fabian@vivian.w.open.de (Fabian Hoppe)\\n...</td>\n",
       "      <td>{'NORP': ['fabian'], 'ORG': ['nntp'], 'PERSON'...</td>\n",
       "      <td>{'NORP': ['fabian'], 'PERSON': ['fabian hoppe'...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           News Data  \\\n",
       "0  From: rcollins@ns.encore.com (Roger Collins)\\n...   \n",
       "1  From: baalke@kelvin.jpl.nasa.gov (Ron Baalke)\\...   \n",
       "2  Distribution: world\\nFrom: David_A._Schnider@b...   \n",
       "3  From: brian@nostromo.NoSubdomain.NoDomain (Bri...   \n",
       "4  From: fabian@vivian.w.open.de (Fabian Hoppe)\\n...   \n",
       "\n",
       "                                     Actual_Entities  \\\n",
       "0  {'PERSON': ['roger collins', 'clinton', 'steve...   \n",
       "1  {'PERSON': ['daniel burstein', 'ron baalke', '...   \n",
       "2  {'ORG': ['mac vga', 'sony cpd', 'cpd', 'trinit...   \n",
       "3  {'PERSON': ['brian colaric sun', 'brian colari...   \n",
       "4  {'NORP': ['fabian'], 'ORG': ['nntp'], 'PERSON'...   \n",
       "\n",
       "                                    Refined_Entities  \n",
       "0  {'PERSON': ['steve hendricks', 'roger collins'...  \n",
       "1  {'PERSON': ['ron baalke', 'daniel burstein'], ...  \n",
       "2  {'ORG': ['sony'], 'LOC': ['macs'], 'PERSON': [...  \n",
       "3                                   {'ORG': ['os2']}  \n",
       "4  {'NORP': ['fabian'], 'PERSON': ['fabian hoppe'...  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test = pd.read_csv('/usercode/test_data.csv')\n",
    "\n",
    "# Fixing the Actual_Entities column\n",
    "def dict_fix(val):\n",
    "    return ast.literal_eval(val)\n",
    "\n",
    "df_test[\"Actual_Entities\"] = df_test[\"Actual_Entities\"].apply(dict_fix)\n",
    "\n",
    "df_test['Processed_Data'] = df_test['News Data'].apply(preprocess_data)\n",
    "\n",
    "df_test['entities'] = extract_entities_pipe(df_test['Processed_Data'])\n",
    "\n",
    "df_test['Refined_Entities'] = df_test['entities'].apply(aggregate_entities)\n",
    "\n",
    "df_test.drop([\"Processed_Data\",\"entities\"], axis=1, inplace=True)\n",
    "\n",
    "df_test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 15: Compute the Evaluation Metrics\n",
    "\n",
    "In this task, you’ll assess the effectiveness of the entity recognition model by calculating precision, recall, and F1-score. These metrics provide\n",
    "insight into the model’s accuracy and its ability to identify relevant entities that are crucial for fine-tuning and enhancing model performance.\n",
    "\n",
    "To complete this task, perform the following steps:\n",
    "\n",
    "1. Create a calculate_entity_metrics() named function which calculates precision, recall, and F1-score for each row of the test_data with the help of\n",
    "Actual_Entities and Refined_Entities.\n",
    "\n",
    "2. Calculate the average precision, recall, and F1-score (precision, recall, and f1, respectively) for test_data and print them.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_entity_metrics(actual, predicted):\n",
    "    precision, recall, f1 = 0, 0, 0\n",
    "    num_entities = len(actual)\n",
    "\n",
    "    for ent_type in actual:\n",
    "        actual_entities = set(actual.get(ent_type, []))\n",
    "        predicted_entities = set(predicted.get(ent_type, []))\n",
    "\n",
    "        true_positives = len(actual_entities & predicted_entities)\n",
    "        false_positives = len(predicted_entities - actual_entities)\n",
    "        false_negatives = len(actual_entities - predicted_entities)\n",
    "\n",
    "        precision_part = true_positives / (true_positives + false_positives) if true_positives + false_positives > 0 else 0\n",
    "        recall_part = true_positives / (true_positives + false_negatives) if true_positives + false_negatives > 0 else 0\n",
    "        \n",
    "        precision += precision_part\n",
    "        recall += recall_part\n",
    "\n",
    "     # Handle the case where there are no entities\n",
    "    if num_entities > 0:\n",
    "        precision /= num_entities\n",
    "        recall /= num_entities\n",
    "        if precision + recall > 0:\n",
    "            f1 = 2 * precision * recall / (precision + recall)\n",
    "    else:\n",
    "        precision, recall, f1 = 0, 0, 0  \n",
    "\n",
    "    return precision, recall, f1\n",
    "\n",
    "\n",
    "metrics = df_test.apply(lambda row: calculate_entity_metrics(row['Actual_Entities'], row['Refined_Entities']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Precision: 0.2637706360265627\n",
      "Average Recall: 0.15532456660089272\n",
      "Average f1: 0.1866298513435013\n"
     ]
    }
   ],
   "source": [
    "precisions, recalls, f1s = zip(*metrics)\n",
    "average_precision = sum(precisions) / len(precisions)\n",
    "average_recall = sum(recalls) / len(recalls)\n",
    "average_f1 = sum(f1s) / len(f1s)\n",
    "\n",
    "print(f'Average Precision: {average_precision}')\n",
    "print(f'Average Recall: {average_recall}')\n",
    "print(f'Average f1: {average_f1}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
